---
title: "Evaluating Variance in Visual Question Answering Benchmarks"
authors:
- Nikitha SR


date: "2025-08-04T00:00:00Z"
doi: ""

publishDate: "2025-08-04T00:00:00Z"

publication_types: ["conference"]

publication: "ICCV Workshop"
publication_short: "ICCV Workshop"

abstract: "Multimodal large language models (MLLMs) have emerged as powerful tools for visual question answering (VQA), enabling reasoning and contextual understanding across visual and textual modalities. Despite their advancements, the evaluation of MLLMs on VQA benchmarks often relies on point estimates, overlooking the significant variance in performance caused by factors such as stochastic model outputs, training seed sensitivity, and hyperparameter configurations. This paper critically examines these issues by analyzing variance across 14 widely used VQA benchmarks, covering diverse tasks such as visual reasoning, text understanding, and commonsense reasoning. We systematically study the impact of training seed, framework non-determinism, model scale, and extended instruction finetuning on performance variability. Additionally, we explore Cloze-style evaluation as an alternate assessment strategy, studying its effectiveness in reducing stochasticity and improving reliability across benchmarks. Our findings highlight the limitations of current evaluation practices and advocate for variance-aware methodologies to foster more robust and reliable development of MLLMs."

tags:
- Multimodal Large Languange models
- Visual question answering
- Vision language models
featured: false

links:
url_pdf: "https://arxiv.org/abs/2508.02645"
url_code: ""
url_dataset: ""
url_poster: ""
url_project: ""
url_slides: ""
url_source: ""
url_video: ""


projects: []
slides: ""
---

